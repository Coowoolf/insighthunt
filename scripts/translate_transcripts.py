#!/usr/bin/env python3
"""
InsightHunt - Transcript Translation Pipeline
Translates 297 podcast transcripts from English to Chinese.

Usage: python3 scripts/translate_transcripts.py --count 10
"""

import requests
import json
import os
import time
import argparse
from pathlib import Path
from typing import Dict

# Antigravity Proxy Configuration - Claude 4.5 Sonnet (using requests directly)
API_URL = "http://127.0.0.1:8045/v1/messages"
API_KEY = "sk-46809d8691dd4542add62c1516537169"
MODEL = "claude-sonnet-4-5"

# Paths
TRANSCRIPTS_DIR = Path("/Users/yaoguanghua/Downloads/Lenny_Podcast_Transcripts")
OUTPUT_DIR = Path("/Users/yaoguanghua/Projects/Skills/insighthunt/data/transcripts")


def translate_chunk(text: str, context: str = "") -> str:
    """Translate a chunk of transcript text to Chinese with retry logic"""
    
    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¸­è‹±ç¿»è¯‘ä¸“å®¶ï¼Œä¸“æ³¨äºäº§å“ç®¡ç†å’Œåˆ›ä¸šé¢†åŸŸçš„æ’­å®¢å†…å®¹ç¿»è¯‘ã€‚

è¯·å°†ä»¥ä¸‹æ’­å®¢è½¬å½•æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ã€‚

ç¿»è¯‘åŸåˆ™ï¼š
1. ä¿æŒå¯¹è¯çš„è‡ªç„¶æµç•…ï¼Œåƒä¸­æ–‡åŸç”Ÿå¯¹è¯ä¸€æ ·
2. ä¸“ä¸šæœ¯è¯­ä¿ç•™è‹±æ–‡ç¼©å†™ï¼ˆå¦‚ PMF, OKR, MVP, A/B Testï¼‰
3. äººåå’Œå…¬å¸åä¿æŒè‹±æ–‡
4. ä¿ç•™è¯´è¯äººçš„è¯­æ°”å’Œé£æ ¼
5. é€‚å½“å¤„ç†å£è¯­åŒ–è¡¨è¾¾ï¼Œä½¿å…¶æ›´ç¬¦åˆä¸­æ–‡ä¹ æƒ¯

{f"ä¸Šä¸‹æ–‡èƒŒæ™¯: {context}" if context else ""}

åŸæ–‡:
{text}

è¯·ç›´æ¥è¾“å‡ºç¿»è¯‘åçš„ä¸­æ–‡ï¼Œä¸éœ€è¦é¢å¤–è§£é‡Šã€‚"""

    headers = {
        'Content-Type': 'application/json',
        'x-api-key': API_KEY,
        'anthropic-version': '2023-06-01'
    }
    data = {
        'model': MODEL,
        'max_tokens': 8000,
        'messages': [{'role': 'user', 'content': prompt}]
    }

    max_retries = 5
    for attempt in range(max_retries):
        try:
            response = requests.post(API_URL, headers=headers, json=data, timeout=120)
            if response.status_code == 200:
                return response.json()['content'][0]['text'].strip()
            else:
                error_msg = response.text[:200]
                if response.status_code in [502, 503, 429]:
                    wait_time = (2 ** attempt) * 5
                    print(f"    âš ï¸ API error {response.status_code} (attempt {attempt+1}/{max_retries}): {error_msg}")
                    print(f"    â³ Waiting {wait_time}s before retry...")
                    time.sleep(wait_time)
                else:
                    print(f"    âš ï¸ Translation error {response.status_code}: {error_msg}")
                    return ""
        except Exception as e:
            wait_time = (2 ** attempt) * 5
            print(f"    âš ï¸ Request error (attempt {attempt+1}/{max_retries}): {e}")
            print(f"    â³ Waiting {wait_time}s before retry...")
            time.sleep(wait_time)
    
    print(f"    âŒ Failed after {max_retries} attempts")
    return ""


def translate_transcript(filepath: Path) -> Dict:
    """Translate a complete transcript file"""
    
    with open(filepath, 'r', encoding='utf-8') as f:
        text_en = f.read()
    
    # Get guest name from filename
    guest_name = filepath.stem
    
    # Split into chunks of ~10000 characters for faster translation
    # Larger chunks = fewer API calls = faster processing
    chunk_size = 10000
    overlap = 100  # Small overlap for context continuity
    
    chunks = []
    start = 0
    while start < len(text_en):
        end = min(start + chunk_size, len(text_en))
        # Try to end at a sentence boundary
        if end < len(text_en):
            for marker in ['. ', '.\n', '? ', '?\n', '! ', '!\n']:
                last_period = text_en[start:end].rfind(marker)
                if last_period > chunk_size * 0.7:
                    end = start + last_period + len(marker)
                    break
        chunks.append(text_en[start:end])
        start = end - overlap if end < len(text_en) else end
    
    print(f"    ğŸ“„ {len(chunks)} chunks to translate")
    
    translated_chunks = []
    for i, chunk in enumerate(chunks):
        context = f"å˜‰å®¾: {guest_name}, ç¬¬ {i+1}/{len(chunks)} æ®µ"
        print(f"    ğŸ”„ Translating chunk {i+1}/{len(chunks)}...")
        translated = translate_chunk(chunk, context)
        if translated:
            translated_chunks.append(translated)
        time.sleep(1)  # Rate limiting
    
    text_zh = "\n\n".join(translated_chunks)
    
    return {
        "guest": guest_name,
        "en": text_en,
        "zh": text_zh,
        "chunks_count": len(chunks)
    }


def get_processed_transcripts() -> set:
    """Get set of already translated transcripts (those with actual zh content)"""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    processed = set()
    for f in OUTPUT_DIR.glob("*.json"):
        try:
            with open(f, 'r', encoding='utf-8') as fp:
                data = json.load(fp)
                # Check if actually translated (has zh content)
                if data.get('zh') and len(data.get('zh', '')) > 100:
                    processed.add(f.stem)
        except:
            pass
    return processed


def main():
    parser = argparse.ArgumentParser(description='Translate podcast transcripts to Chinese')
    parser.add_argument('--count', type=int, default=10, help='Number to process')
    parser.add_argument('--list', action='store_true', help='List unprocessed and exit')
    args = parser.parse_args()
    
    print("ğŸŒ InsightHunt Transcript Translation")
    print("=" * 50)
    
    # Get all transcripts
    all_transcripts = sorted(TRANSCRIPTS_DIR.glob("*.txt"))
    processed = get_processed_transcripts()
    unprocessed = [t for t in all_transcripts if t.stem not in processed]
    
    print(f"\nğŸ“Š Status: {len(processed)}/{len(all_transcripts)} translated")
    print(f"   Remaining: {len(unprocessed)}")
    
    if args.list:
        print("\nUnprocessed transcripts:")
        for i, t in enumerate(unprocessed[:20], 1):
            print(f"  {i}. {t.stem}")
        if len(unprocessed) > 20:
            print(f"  ... and {len(unprocessed) - 20} more")
        return
    
    if not unprocessed:
        print("ğŸ‰ All transcripts translated!")
        return
    
    batch = unprocessed[:args.count]
    print(f"\nğŸ¯ Processing {len(batch)} transcripts\n")
    
    success = 0
    for i, filepath in enumerate(batch):
        print(f"\n[{len(processed) + i + 1}/{len(all_transcripts)}] {filepath.stem}")
        
        try:
            data = translate_transcript(filepath)
            
            # Save
            output_path = OUTPUT_DIR / f"{filepath.stem}.json"
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            char_count_en = len(data['en'])
            char_count_zh = len(data['zh'])
            print(f"    âœ… Done: {char_count_en:,} â†’ {char_count_zh:,} chars")
            success += 1
            
        except Exception as e:
            print(f"    âŒ Error: {e}")
        
        time.sleep(2)
    
    print(f"\n{'='*50}")
    print(f"âœ… Batch Complete: {success}/{len(batch)}")
    print(f"ğŸ“Œ Next: python3 scripts/translate_transcripts.py --count {args.count}")


if __name__ == "__main__":
    main()
