#!/usr/bin/env python3
"""
InsightHunt - Transcript Translation Pipeline
Translates 297 podcast transcripts from English to Chinese.

Usage: python3 scripts/translate_transcripts.py --count 10
"""

from anthropic import Anthropic
import json
import os
import time
import argparse
from pathlib import Path
from typing import Dict

# Antigravity Proxy Configuration
client = Anthropic(
    base_url="http://127.0.0.1:8045",
    api_key="sk-cbb33b67c7f14a208a67aa705ebf80ee"
)

MODEL = "gemini-3-pro-high"

# Paths
TRANSCRIPTS_DIR = Path("/Users/yaoguanghua/Downloads/Lenny_Podcast_Transcripts")
OUTPUT_DIR = Path("/Users/yaoguanghua/Projects/Skills/insighthunt/data/transcripts")


def translate_chunk(text: str, context: str = "") -> str:
    """Translate a chunk of transcript text to Chinese"""
    
    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¸­è‹±ç¿»è¯‘ä¸“å®¶ï¼Œä¸“æ³¨äºäº§å“ç®¡ç†å’Œåˆ›ä¸šé¢†åŸŸçš„æ’­å®¢å†…å®¹ç¿»è¯‘ã€‚

è¯·å°†ä»¥ä¸‹æ’­å®¢è½¬å½•æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ã€‚

ç¿»è¯‘åŸåˆ™ï¼š
1. ä¿æŒå¯¹è¯çš„è‡ªç„¶æµç•…ï¼Œåƒä¸­æ–‡åŸç”Ÿå¯¹è¯ä¸€æ ·
2. ä¸“ä¸šæœ¯è¯­ä¿ç•™è‹±æ–‡ç¼©å†™ï¼ˆå¦‚ PMF, OKR, MVP, A/B Testï¼‰
3. äººåå’Œå…¬å¸åä¿æŒè‹±æ–‡
4. ä¿ç•™è¯´è¯äººçš„è¯­æ°”å’Œé£æ ¼
5. é€‚å½“å¤„ç†å£è¯­åŒ–è¡¨è¾¾ï¼Œä½¿å…¶æ›´ç¬¦åˆä¸­æ–‡ä¹ æƒ¯

{f"ä¸Šä¸‹æ–‡èƒŒæ™¯: {context}" if context else ""}

åŸæ–‡:
{text}

è¯·ç›´æ¥è¾“å‡ºç¿»è¯‘åçš„ä¸­æ–‡ï¼Œä¸éœ€è¦é¢å¤–è§£é‡Šã€‚"""

    try:
        response = client.messages.create(
            model=MODEL,
            max_tokens=8000,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.content[0].text.strip()
    except Exception as e:
        print(f"    âš ï¸ Translation error: {e}")
        return ""


def translate_transcript(filepath: Path) -> Dict:
    """Translate a complete transcript file"""
    
    with open(filepath, 'r', encoding='utf-8') as f:
        text_en = f.read()
    
    # Get guest name from filename
    guest_name = filepath.stem
    
    # Split into chunks of ~3000 characters for translation
    # This ensures we stay within token limits while maintaining context
    chunk_size = 3000
    overlap = 200  # Overlap for context continuity
    
    chunks = []
    start = 0
    while start < len(text_en):
        end = min(start + chunk_size, len(text_en))
        # Try to end at a sentence boundary
        if end < len(text_en):
            for marker in ['. ', '.\n', '? ', '?\n', '! ', '!\n']:
                last_period = text_en[start:end].rfind(marker)
                if last_period > chunk_size * 0.7:
                    end = start + last_period + len(marker)
                    break
        chunks.append(text_en[start:end])
        start = end - overlap if end < len(text_en) else end
    
    print(f"    ğŸ“„ {len(chunks)} chunks to translate")
    
    translated_chunks = []
    for i, chunk in enumerate(chunks):
        context = f"å˜‰å®¾: {guest_name}, ç¬¬ {i+1}/{len(chunks)} æ®µ"
        print(f"    ğŸ”„ Translating chunk {i+1}/{len(chunks)}...")
        translated = translate_chunk(chunk, context)
        if translated:
            translated_chunks.append(translated)
        time.sleep(1)  # Rate limiting
    
    text_zh = "\n\n".join(translated_chunks)
    
    return {
        "guest": guest_name,
        "en": text_en,
        "zh": text_zh,
        "chunks_count": len(chunks)
    }


def get_processed_transcripts() -> set:
    """Get set of already translated transcripts"""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    return {f.stem for f in OUTPUT_DIR.glob("*.json")}


def main():
    parser = argparse.ArgumentParser(description='Translate podcast transcripts to Chinese')
    parser.add_argument('--count', type=int, default=10, help='Number to process')
    parser.add_argument('--list', action='store_true', help='List unprocessed and exit')
    args = parser.parse_args()
    
    print("ğŸŒ InsightHunt Transcript Translation")
    print("=" * 50)
    
    # Get all transcripts
    all_transcripts = sorted(TRANSCRIPTS_DIR.glob("*.txt"))
    processed = get_processed_transcripts()
    unprocessed = [t for t in all_transcripts if t.stem not in processed]
    
    print(f"\nğŸ“Š Status: {len(processed)}/{len(all_transcripts)} translated")
    print(f"   Remaining: {len(unprocessed)}")
    
    if args.list:
        print("\nUnprocessed transcripts:")
        for i, t in enumerate(unprocessed[:20], 1):
            print(f"  {i}. {t.stem}")
        if len(unprocessed) > 20:
            print(f"  ... and {len(unprocessed) - 20} more")
        return
    
    if not unprocessed:
        print("ğŸ‰ All transcripts translated!")
        return
    
    batch = unprocessed[:args.count]
    print(f"\nğŸ¯ Processing {len(batch)} transcripts\n")
    
    success = 0
    for i, filepath in enumerate(batch):
        print(f"\n[{len(processed) + i + 1}/{len(all_transcripts)}] {filepath.stem}")
        
        try:
            data = translate_transcript(filepath)
            
            # Save
            output_path = OUTPUT_DIR / f"{filepath.stem}.json"
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            char_count_en = len(data['en'])
            char_count_zh = len(data['zh'])
            print(f"    âœ… Done: {char_count_en:,} â†’ {char_count_zh:,} chars")
            success += 1
            
        except Exception as e:
            print(f"    âŒ Error: {e}")
        
        time.sleep(2)
    
    print(f"\n{'='*50}")
    print(f"âœ… Batch Complete: {success}/{len(batch)}")
    print(f"ğŸ“Œ Next: python3 scripts/translate_transcripts.py --count {args.count}")


if __name__ == "__main__":
    main()
